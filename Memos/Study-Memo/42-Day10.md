## LSTM深入（长短时记忆网络）

- 门结构
  - 对张量范围的控制，电路上变现为乘号
  - 对应一个全连接的权重矩阵
- 4个全连接矩阵（基本CNN是3个）
  - 输入门--影响记忆单元
  - 遗忘门--影响记忆单元
  - 记忆单元
  - 输出门
- 参数多，速度慢

## GRU

- 对LSTM做了简化

  - 合并记忆状态C与隐藏状态H

- 只有两个门，3个矩阵

  - 更新门
    - 合并了输入门和遗忘门
  - 重置门

- 参数少，速度快

  ## 循环网络实现

- RNN-cell

  - 参数num_units----神经元个数
  - call函数是一个时间步的实现
    - 通过in、s求出out、s+

## Keras RNN类

- 有内置的常用RNN层，也可以自定义

- 内置RNN层

  - keras.layers.RNN

  - keras.layers.SimpleRNN---将上个时间步的输出作为下个时间步的输入

  - keras.layers.LSTM

  - keras.layers.GRU

  - 默认情况下输出的形状（批次大小，神经元个数） 若设置"return_sequences=true"，输出形状为（批次大小，timesteps,神经元个数）

    ## Keras RNN Cell

- RNN处理整批输入序列，RNN cell处理单个时间步，可以使用cell创建RNN

- 内置

  - keras.layers.SimpleRNNCell

  - keras.layers.LSTMCell

  - keras.layers.GRUCell

    ## 跨批次的状态性

- 参数中“stateful=ture”保留状态，只适用于相同的RNN之间

## 用数字表示文本--编码

- 独热编码（one-hot）---即单点编码
- 用唯一的数字来编码每个单词
  - 例如用3编码cat
- 单词嵌入向量
  - 系统自动生成，无需手动编码
  - 相似的单词会有相似的编码
  - 嵌入向量层
    - 相当于
    - layers.Embedding（表大小，嵌入向量维度）