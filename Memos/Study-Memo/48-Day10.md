# 第十周

+ 回顾了多层全连接网络FCN

+ LSTM深入（长短时记忆网络）

  遗忘门，控制记忆单元的门，全连接矩阵WF

  记忆单元是LSTM的核心，作用是对时间的一个积分器

  损失函数的导数如果不能通过$c_t$~进行反向传播，可以通过$c_(t-1)$反向传播，增加了一条路径

+ 门控循环单元全程GRU

  是LSTM的变体，合并了cell和隐藏状态

  合并了输入门和遗忘门，只有更新门和重置门

+ 循环网络应用—语音识别、图片注解、单词嵌入向量、文本分类、文本生成
+ 神经网路机器翻译简介
+ transformer模型和神经图灵机简介。

+ rnn-cell抽象

  输出完全取决于state和当前输入，o(output)+s(new_state)<=i(input)+s(state)

  用cell 生成rnn层和rnn类

+ 单词嵌入向量，在表中查找对应的密集向量

  给定一个输入，返回3D浮点张量。

