# Keras



Keras总体

Keras层和模型

Keras训练和评估

Keras顺序模型

Keras模型的保存和加载

Keras循环训练

Keras循环网络

Keras函数式API

Keras自建fit()内部操作

Keras遮盖与填充

Keras迁移学习与细调



# 人工神经网络

- 前馈网络 feedforward

  （FNN）计算快

  - 多层全连接网络（FCN）、多层感知机（MLP）、多个密集层网络（dense）
  - 卷积网络（CNN）（局部连接）：计算快，可并行化

- 反馈网络
  - 循环网络（RNN）：当前时刻的输出，可以作为下一时刻的输入

- 记忆网络



# Keras

一种描述语言

一套高级API（应用程序接口），用于快速搭建深度神经网络

封装深度学习的底层框架，形成一致的api

底层框架，包括：TensorFlow，Theano和CNTK



# Keras 核心概念

## Model

核心的数据结构叫模型（Model）



A `Sequential` model is appropriate for **a plain stack of layers** where each layer has **exactly one input tensor and one output tensor**.

A Sequential model is **not appropriate** when:

- Your model has multiple inputs or multiple outputs
- Any of your layers has multiple inputs or multiple outputs
- You need to do layer sharing
- You want non-linear topology (e.g. a residual connection, a multi-branch model)



## Layer

`tf.keras.layers`

Dense

Activation

Dropout

Flatten

Reshape

Permute

RepeatVector

Lambda

Activity 



## 激活函数

`tf.keras.activations`

softmax

elu（指数线性单元）

softplus

softsign

relu

tanh

sigmoid

hard_sigmoid

linear



## 优化器 optimizers

Optimizer基类

SGD（最简单的优化器之一，带有动量参数（momentum））

RMSprop、Adagrad、Adam 都是带有自适应调整学习率的算法的优化器

Adagrad保存每个时刻的梯度计算值，并计算出每个维度上的学习率

Adadelta

Adamax

Nadam - 采用Nesterov加速梯度算法的优化器

​	基于之前所有的梯度（全局的信息）更新模型参数

​	可以进一步提升训练收敛的速度



mnist数据集











# Keras 使用流程

Define

Compile

Fit

Evaluate

Predict

Load/Save



# Keras训练和评估



## 手写数字识别





## 创建顺序模型

- 预先指定输入形状
- `model.summary()` 可以显示创建的模型的摘要（基本网络结构的形状）



# Keras Layers/Model

## Layer 类：权重和部分计算的组合

一个中心抽象是Layer类

- layer层封装了状态（层的“权重”）和从输入到输出的转换（“调用” 层的向前传递）



# Eager Execution



# 基础补充

`tf.layers.dense()`的使用

    tf.layers.dense(
    inputs,
    units,
    activation=None,
    use_bias=True,
    kernel_initializer=None,  ##卷积核的初始化器
    bias_initializer=tf.zeros_initializer(),  ##偏置项的初始化器，默认初始化为0
    kernel_regularizer=None,    ##卷积核的正则化，可选
    bias_regularizer=None,    ##偏置项的正则化，可选
    activity_regularizer=None,   ##输出的正则化函数
    kernel_constraint=None,   
    bias_constraint=None,
    trainable=True,
    name=None,  ##层的名字
    reuse=None  ##是否重复使用参数
    )



部分参数解释：

inputs：输入该网络层的数据

units：输出的维度大小，改变inputs的最后一维

activation：激活函数，即神经网络的非线性变化

use_bias：使用bias为True（默认使用），不用bias改成False即可，是否使用偏置项

trainable=True:表明该层的参数是否参与训练。如果为真则变量加入到图集合中

 GraphKeys.TRAINABLE_VARIABLES (see tf.Variable)

在其他网站上看到的使用现象

dense1 = tf.layers.dense(inputs=pool3, units=1024, activation=tf.nn.relu，                                

                                           kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))

# 全连接层

dense1 = tf.layers.dense(inputs=pool3, units=1024, activation=tf.nn.relu)

dense2= tf.layers.dense(inputs=dense1, units=512, activation=tf.nn.relu)

logits= tf.layers.dense(inputs=dense2, units=10, activation=None)
————————————————
版权声明：本文为CSDN博主「凤玲fly」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/yangfengling1023/article/details/81774580

